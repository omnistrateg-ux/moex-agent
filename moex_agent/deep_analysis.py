"""
MOEX Agent - –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

–ê–Ω–∞–ª–∏–∑ 4 –ª–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.

Usage:
    python -m moex_agent.deep_analysis
"""
from __future__ import annotations

import argparse
import json
import logging
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from scipy import stats

from .config import load_config
from .features import build_feature_frame
from .storage import connect

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger("moex_agent.deep_analysis")


def load_all_candles(conn) -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–≤–µ—á–µ–π."""
    logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ—á–µ–π...")
    q = """
    SELECT secid, ts, open, high, low, close, value, volume
    FROM candles
    WHERE interval = 1
    ORDER BY ts, secid
    """
    df = pd.read_sql_query(q, conn)
    df["ts"] = pd.to_datetime(df["ts"], utc=True)
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df):,} —Å–≤–µ—á–µ–π")
    return df


def analyze_market_regimes(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤ (–±—ã—á–∏–π/–º–µ–¥–≤–µ–∂–∏–π/–±–æ–∫–æ–≤–æ–π)."""
    logger.info("–ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ä–µ–∂–∏–º–æ–≤...")

    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º
    daily = df.groupby([df["ts"].dt.date, "secid"]).agg({
        "close": "last",
        "value": "sum",
        "volume": "sum"
    }).reset_index()

    # –°—á–∏—Ç–∞–µ–º –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å (—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –±—É–º–∞–≥–∞–º)
    market_daily = daily.groupby("ts").agg({
        "close": "mean",
        "value": "sum"
    }).reset_index()
    market_daily.columns = ["date", "avg_close", "total_value"]

    # –†–∞—Å—á—ë—Ç returns
    market_daily["return"] = market_daily["avg_close"].pct_change()

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º—ã (20-–¥–Ω–µ–≤–Ω–æ–µ –æ–∫–Ω–æ)
    market_daily["sma20"] = market_daily["avg_close"].rolling(20).mean()
    market_daily["sma50"] = market_daily["avg_close"].rolling(50).mean()
    market_daily["volatility"] = market_daily["return"].rolling(20).std()

    # –†–µ–∂–∏–º: bull (sma20 > sma50), bear (sma20 < sma50), sideways (–±–ª–∏–∑–∫–æ)
    def classify_regime(row):
        if pd.isna(row["sma20"]) or pd.isna(row["sma50"]):
            return "unknown"
        diff_pct = (row["sma20"] - row["sma50"]) / row["sma50"] * 100
        if diff_pct > 2:
            return "bull"
        elif diff_pct < -2:
            return "bear"
        else:
            return "sideways"

    market_daily["regime"] = market_daily.apply(classify_regime, axis=1)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∂–∏–º–∞–º
    regime_stats = market_daily.groupby("regime").agg({
        "return": ["mean", "std", "count"],
        "volatility": "mean"
    })

    regimes = {
        "bull_days": int((market_daily["regime"] == "bull").sum()),
        "bear_days": int((market_daily["regime"] == "bear").sum()),
        "sideways_days": int((market_daily["regime"] == "sideways").sum()),
        "total_days": len(market_daily),
        "avg_daily_return": float(market_daily["return"].mean() * 100),
        "avg_volatility": float(market_daily["volatility"].mean() * 100),
    }

    logger.info(f"–†–µ–∂–∏–º—ã: bull={regimes['bull_days']}, bear={regimes['bear_days']}, sideways={regimes['sideways_days']}")

    return regimes


def analyze_seasonality(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ (–¥–Ω–∏ –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü—ã, —á–∞—Å—ã)."""
    logger.info("–ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏...")

    df = df.copy()
    df["hour"] = df["ts"].dt.hour
    df["weekday"] = df["ts"].dt.weekday
    df["month"] = df["ts"].dt.month
    df["return"] = df.groupby("secid")["close"].pct_change()

    # –ü–æ —á–∞—Å–∞–º
    hourly = df.groupby("hour")["return"].agg(["mean", "std", "count"])
    best_hour = hourly["mean"].idxmax()
    worst_hour = hourly["mean"].idxmin()

    # –ü–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
    weekday_names = ["–ü–Ω", "–í—Ç", "–°—Ä", "–ß—Ç", "–ü—Ç", "–°–±", "–í—Å"]
    daily = df.groupby("weekday")["return"].agg(["mean", "std", "count"])
    best_weekday = daily["mean"].idxmax()
    worst_weekday = daily["mean"].idxmin()

    # –ü–æ –º–µ—Å—è—Ü–∞–º
    monthly = df.groupby("month")["return"].agg(["mean", "std", "count"])
    best_month = monthly["mean"].idxmax()
    worst_month = monthly["mean"].idxmin()

    seasonality = {
        "best_hour": int(best_hour),
        "worst_hour": int(worst_hour),
        "best_weekday": weekday_names[best_weekday],
        "worst_weekday": weekday_names[worst_weekday],
        "best_month": int(best_month),
        "worst_month": int(worst_month),
        "hourly_returns": {int(h): float(r * 100) for h, r in hourly["mean"].items()},
        "weekday_returns": {weekday_names[w]: float(r * 100) for w, r in daily["mean"].items()},
        "monthly_returns": {int(m): float(r * 100) for m, r in monthly["mean"].items()},
    }

    logger.info(f"–õ—É—á—à–∏–π —á–∞—Å: {best_hour}:00, –õ—É—á—à–∏–π –¥–µ–Ω—å: {weekday_names[best_weekday]}, –õ—É—á—à–∏–π –º–µ—Å—è—Ü: {best_month}")

    return seasonality


def analyze_tickers(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤."""
    logger.info("–ê–Ω–∞–ª–∏–∑ —Ç–∏–∫–µ—Ä–æ–≤...")

    ticker_stats = {}

    for secid in df["secid"].unique():
        ticker_df = df[df["secid"] == secid].copy()
        ticker_df["return"] = ticker_df["close"].pct_change()

        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        returns = ticker_df["return"].dropna()
        if len(returns) < 100:
            continue

        # Sharpe ratio (annualized)
        sharpe = returns.mean() / returns.std() * np.sqrt(252 * 60 * 9)  # 9 —á–∞—Å–æ–≤ —Ç–æ—Ä–≥–æ–≤

        # Max drawdown
        cumulative = (1 + returns).cumprod()
        peak = cumulative.cummax()
        drawdown = (cumulative - peak) / peak
        max_drawdown = drawdown.min()

        # Volatility (annualized)
        volatility = returns.std() * np.sqrt(252 * 60 * 9)

        # –°—Ä–µ–¥–Ω–∏–π –æ–±–æ—Ä–æ—Ç
        avg_turnover = ticker_df["value"].mean()

        ticker_stats[secid] = {
            "total_candles": len(ticker_df),
            "avg_return": float(returns.mean() * 100),
            "volatility": float(volatility * 100),
            "sharpe": float(sharpe),
            "max_drawdown": float(max_drawdown * 100),
            "avg_turnover": float(avg_turnover),
            "skewness": float(stats.skew(returns)),
            "kurtosis": float(stats.kurtosis(returns)),
        }

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ Sharpe
    sorted_tickers = sorted(ticker_stats.items(), key=lambda x: x[1]["sharpe"], reverse=True)

    logger.info(f"–¢–æ–ø-5 –ø–æ Sharpe: {[t[0] for t in sorted_tickers[:5]]}")

    return {
        "tickers": ticker_stats,
        "top_sharpe": [t[0] for t in sorted_tickers[:10]],
        "worst_sharpe": [t[0] for t in sorted_tickers[-10:]],
    }


def analyze_correlations(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —Ç–∏–∫–µ—Ä–∞–º–∏."""
    logger.info("–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")

    # Pivot —Ç–∞–±–ª–∏—Ü–∞ —Å returns
    pivot = df.pivot_table(index="ts", columns="secid", values="close")
    returns = pivot.pct_change().dropna()

    # –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    corr_matrix = returns.corr()

    # –°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
    avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, 1)].mean()

    # –ù–∞–∏–º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã (–¥–ª—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    corr_pairs = []
    for i, t1 in enumerate(corr_matrix.columns):
        for j, t2 in enumerate(corr_matrix.columns):
            if i < j:
                corr_pairs.append((t1, t2, corr_matrix.loc[t1, t2]))

    corr_pairs.sort(key=lambda x: x[2])

    logger.info(f"–°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {avg_corr:.3f}")

    return {
        "avg_correlation": float(avg_corr),
        "least_correlated": [(p[0], p[1], float(p[2])) for p in corr_pairs[:10]],
        "most_correlated": [(p[0], p[1], float(p[2])) for p in corr_pairs[-10:]],
    }


def analyze_patterns(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (momentum, mean reversion)."""
    logger.info("–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")

    results = {
        "momentum": {},
        "mean_reversion": {},
    }

    for secid in df["secid"].unique()[:10]:  # –¢–æ–ø 10 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        ticker_df = df[df["secid"] == secid].copy()
        ticker_df["return"] = ticker_df["close"].pct_change()
        ticker_df["prev_return_5"] = ticker_df["return"].rolling(5).sum().shift(1)
        ticker_df["next_return_5"] = ticker_df["return"].rolling(5).sum().shift(-5)

        valid = ticker_df.dropna(subset=["prev_return_5", "next_return_5"])

        if len(valid) < 100:
            continue

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –ø—Ä–æ—à–ª—ã–º–∏ –∏ –±—É–¥—É—â–∏–º–∏ returns
        corr = valid["prev_return_5"].corr(valid["next_return_5"])

        # Momentum: –ø–æ—Å–ª–µ —Ä–æ—Å—Ç–∞ - —Ä–æ—Å—Ç?
        up_prev = valid[valid["prev_return_5"] > 0]
        if len(up_prev) > 0:
            momentum_signal = (up_prev["next_return_5"] > 0).mean()
        else:
            momentum_signal = 0.5

        # Mean reversion: –ø–æ—Å–ª–µ –ø–∞–¥–µ–Ω–∏—è - —Ä–æ—Å—Ç?
        down_prev = valid[valid["prev_return_5"] < -0.01]
        if len(down_prev) > 0:
            reversion_signal = (down_prev["next_return_5"] > 0).mean()
        else:
            reversion_signal = 0.5

        results["momentum"][secid] = float(momentum_signal)
        results["mean_reversion"][secid] = float(reversion_signal)

    # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    avg_momentum = np.mean(list(results["momentum"].values()))
    avg_reversion = np.mean(list(results["mean_reversion"].values()))

    results["avg_momentum_signal"] = float(avg_momentum)
    results["avg_reversion_signal"] = float(avg_reversion)

    logger.info(f"Momentum signal: {avg_momentum:.2%}, Mean reversion: {avg_reversion:.2%}")

    return results


def analyze_volume_patterns(df: pd.DataFrame) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–±—ä—ë–º–∞."""
    logger.info("–ê–Ω–∞–ª–∏–∑ –æ–±—ä—ë–º–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")

    df = df.copy()
    df["return"] = df.groupby("secid")["close"].pct_change()
    df["volume_sma"] = df.groupby("secid")["volume"].transform(lambda x: x.rolling(20).mean())
    df["volume_ratio"] = df["volume"] / df["volume_sma"]

    # –í—ã—Å–æ–∫–∏–π –æ–±—ä—ë–º (> 2x —Å—Ä–µ–¥–Ω–µ–≥–æ)
    high_volume = df[df["volume_ratio"] > 2]
    high_vol_return = high_volume["return"].mean()

    # –ù–∏–∑–∫–∏–π –æ–±—ä—ë–º (< 0.5x —Å—Ä–µ–¥–Ω–µ–≥–æ)
    low_volume = df[df["volume_ratio"] < 0.5]
    low_vol_return = low_volume["return"].mean()

    # Volume spike -> –±—É–¥—É—â–∏–π return
    df["next_return"] = df.groupby("secid")["return"].shift(-5)
    volume_spikes = df[df["volume_ratio"] > 3]
    spike_next_return = volume_spikes["next_return"].mean()

    results = {
        "high_volume_avg_return": float(high_vol_return * 100) if not np.isnan(high_vol_return) else 0,
        "low_volume_avg_return": float(low_vol_return * 100) if not np.isnan(low_vol_return) else 0,
        "volume_spike_next_return": float(spike_next_return * 100) if not np.isnan(spike_next_return) else 0,
        "high_volume_candles": len(high_volume),
        "volume_spike_candles": len(volume_spikes),
    }

    logger.info(f"High volume return: {results['high_volume_avg_return']:.4f}%, Spike next return: {results['volume_spike_next_return']:.4f}%")

    return results


def generate_report(analysis: Dict) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞."""
    report = []
    report.append("=" * 60)
    report.append("–ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó MOEX (4 –ì–û–î–ê)")
    report.append("=" * 60)

    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    report.append(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    report.append(f"   –í—Å–µ–≥–æ —Å–≤–µ—á–µ–π: {analysis['total_candles']:,}")
    report.append(f"   –ü–µ—Ä–∏–æ–¥: {analysis['period']}")
    report.append(f"   –¢–∏–∫–µ—Ä–æ–≤: {analysis['total_tickers']}")

    # –†—ã–Ω–æ—á–Ω—ã–µ —Ä–µ–∂–∏–º—ã
    regimes = analysis["regimes"]
    report.append(f"\nüìà –†–´–ù–û–ß–ù–´–ï –†–ï–ñ–ò–ú–´")
    report.append(f"   –ë—ã—á–∏–π —Ä—ã–Ω–æ–∫: {regimes['bull_days']} –¥–Ω–µ–π ({regimes['bull_days']/regimes['total_days']*100:.1f}%)")
    report.append(f"   –ú–µ–¥–≤–µ–∂–∏–π —Ä—ã–Ω–æ–∫: {regimes['bear_days']} –¥–Ω–µ–π ({regimes['bear_days']/regimes['total_days']*100:.1f}%)")
    report.append(f"   –ë–æ–∫–æ–≤–∏–∫: {regimes['sideways_days']} –¥–Ω–µ–π ({regimes['sideways_days']/regimes['total_days']*100:.1f}%)")
    report.append(f"   –°—Ä–µ–¥–Ω–∏–π –¥–Ω–µ–≤–Ω–æ–π return: {regimes['avg_daily_return']:.4f}%")
    report.append(f"   –°—Ä–µ–¥–Ω—è—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {regimes['avg_volatility']:.2f}%")

    # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å
    season = analysis["seasonality"]
    report.append(f"\nüóì –°–ï–ó–û–ù–ù–û–°–¢–¨")
    report.append(f"   –õ—É—á—à–∏–π —á–∞—Å: {season['best_hour']}:00")
    report.append(f"   –•—É–¥—à–∏–π —á–∞—Å: {season['worst_hour']}:00")
    report.append(f"   –õ—É—á—à–∏–π –¥–µ–Ω—å: {season['best_weekday']}")
    report.append(f"   –•—É–¥—à–∏–π –¥–µ–Ω—å: {season['worst_weekday']}")
    report.append(f"   –õ—É—á—à–∏–π –º–µ—Å—è—Ü: {season['best_month']}")
    report.append(f"   –•—É–¥—à–∏–π –º–µ—Å—è—Ü: {season['worst_month']}")

    # –¢–∏–∫–µ—Ä—ã
    tickers = analysis["tickers"]
    report.append(f"\nüèÜ –õ–£–ß–®–ò–ï –¢–ò–ö–ï–†–´ (–ø–æ Sharpe)")
    for t in tickers["top_sharpe"][:5]:
        stats = tickers["tickers"][t]
        report.append(f"   {t}: Sharpe={stats['sharpe']:.2f}, Vol={stats['volatility']:.1f}%, DD={stats['max_drawdown']:.1f}%")

    report.append(f"\n‚ö†Ô∏è –•–£–î–®–ò–ï –¢–ò–ö–ï–†–´ (–ø–æ Sharpe)")
    for t in tickers["worst_sharpe"][:5]:
        stats = tickers["tickers"][t]
        report.append(f"   {t}: Sharpe={stats['sharpe']:.2f}, Vol={stats['volatility']:.1f}%, DD={stats['max_drawdown']:.1f}%")

    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    corr = analysis["correlations"]
    report.append(f"\nüîó –ö–û–†–†–ï–õ–Ø–¶–ò–ò")
    report.append(f"   –°—Ä–µ–¥–Ω—è—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {corr['avg_correlation']:.3f}")
    report.append(f"   –ù–∞–∏–º–µ–Ω–µ–µ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ:")
    for t1, t2, c in corr["least_correlated"][:3]:
        report.append(f"      {t1} - {t2}: {c:.3f}")

    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã
    patterns = analysis["patterns"]
    report.append(f"\nüîÑ –ü–ê–¢–¢–ï–†–ù–´")
    report.append(f"   Momentum signal: {patterns['avg_momentum_signal']:.1%}")
    report.append(f"   Mean reversion signal: {patterns['avg_reversion_signal']:.1%}")

    # –û–±—ä—ë–º
    volume = analysis["volume"]
    report.append(f"\nüìä –û–ë–™–Å–ú–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´")
    report.append(f"   High volume return: {volume['high_volume_avg_return']:.4f}%")
    report.append(f"   Volume spike ‚Üí next return: {volume['volume_spike_next_return']:.4f}%")

    report.append("\n" + "=" * 60)

    return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(description="–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ MOEX")
    parser.add_argument("--output", default="data/deep_analysis.json", help="Output file")
    args = parser.parse_args()

    cfg = load_config()
    conn = connect(cfg.sqlite_path)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = load_all_candles(conn)

    if len(df) == 0:
        logger.error("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return

    # –ê–Ω–∞–ª–∏–∑
    analysis = {
        "total_candles": len(df),
        "total_tickers": df["secid"].nunique(),
        "period": f"{df['ts'].min().date()} - {df['ts'].max().date()}",
        "generated_at": datetime.now().isoformat(),
    }

    analysis["regimes"] = analyze_market_regimes(df)
    analysis["seasonality"] = analyze_seasonality(df)
    analysis["tickers"] = analyze_tickers(df)
    analysis["correlations"] = analyze_correlations(df)
    analysis["patterns"] = analyze_patterns(df)
    analysis["volume"] = analyze_volume_patterns(df)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(analysis, ensure_ascii=False, indent=2), encoding="utf-8")
    logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞
    report = generate_report(analysis)
    print(report)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    report_path = output_path.with_suffix(".txt")
    report_path.write_text(report, encoding="utf-8")
    logger.info(f"–û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {report_path}")

    conn.close()


if __name__ == "__main__":
    main()
